{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\n",
    "\n",
    "Trong hướng dẫn này, chúng ta sẽ sử dụng các tính năng gọi công cụ (tool calling) của mô hình trò chuyện để trích xuất thông tin có cấu trúc từ văn bản không có cấu trúc. Chúng tôi cũng sẽ trình bày cách sử dụng nhắc nhở (prompt) ít lần trong bối cảnh này để cải thiện hiệu suất.\n",
    "\n",
    "## Content\n",
    "1. Setup\n",
    "2. LangSmith\n",
    "3. The Schema\n",
    "4. The Extractor\n",
    "5. Multiple Entities\n",
    "6. Reference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRAING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, chúng ta cần mô tả thông tin nào chúng ta muốn trích xuất từ ​​văn bản. Chúng ta sẽ sử dụng Pydantic để xác định một lược đồ ví dụ để trích xuất thông tin cá nhân."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    # Doc-string for the entity person\n",
    "    # This doc-string is sent to LLM as the description of the schema person,\n",
    "    # and it can help to improve axtraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field as an `optional` -- this allows the model to decline to extract it\n",
    "    # 2. Each field has a `description` -- this description is used by LLM\n",
    "    # Having a good description can help improve extraction result\n",
    "\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"The color of the person's hair if know\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height measured in meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có hai cách thực hành tốt nhất khi xác định lược đồ: \n",
    "\n",
    "    1. Ghi lại các thuộc tính và chính lược đồ: Thông tin này được gửi đến LLM và được sử dụng để cải thiện chất lượng trích xuất thông tin.\n",
    "    2. Đừng ép LLM phải tạo thông tin! Ở trên, chúng tôi đã sử dụng Optional cho các thuộc tính cho phép LLM đưa ra None nếu không biết câu trả lời.\n",
    "\n",
    "Để có hiệu suất tốt nhất, hãy ghi lại lược đồ thật kỹ và đảm bảo mô hình không bị ép phải trả về kết quả nếu không có thông tin nào cần trích xuất trong văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy tạo một trình trích xuất thông tin bằng cách sử dụng lược đồ đã xác định ở trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm.\"\n",
    "            \"Only extract relevant information from the text.\"\n",
    "            \"If you do not know the value of an attribute asked to extract,\"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta cần sử dụng một mô hình hỗ trợ việc gọi hàm/công cụ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multiple Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong hầu hết các trường hợp, bạn nên trích xuất danh sách các thực thể thay vì một thực thể duy nhất. Điều này có thể dễ dàng đạt được bằng cách sử dụng pydantic bằng cách lồng các mô hình vào nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"The color of the person's hair if known\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height measured in meters\")\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    IMPORTANT\n",
    "Kết quả trích xuất có thể không hoàn hảo ở đây. Đọc tiếp để xem cách sử dụng Ví dụ tham khảo để cải thiện chất lượng trích xuất và xem hướng dẫn cách trích xuất của chúng tôi để biết thêm chi tiết.\n",
    "\n",
    "    https://python.langchain.com/docs/how_to/#extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tip\n",
    "\n",
    "Khi lược đồ hỗ trợ việc trích xuất nhiều thực thể, nó cũng cho phép mô hình không trích xuất bất kỳ thực thể nào nếu không có thông tin liên quan nào trong văn bản bằng cách cung cấp một danh sách trống. Thông thường, đây là một điều tốt! Nó cho phép chỉ định các thuộc tính bắt buộc trên một thực thể mà không nhất thiết buộc mô hình phải phát hiện thực thể này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hành vi của các ứng dụng LLM có thể được điều khiển bằng cách sử dụng lời nhắc ít lần. Đối với các mô hình trò chuyện, điều này có thể diễn ra dưới dạng một chuỗi các cặp thông báo đầu vào và phản hồi thể hiện các hành vi mong muốn. Ví dụ, chúng ta có thể truyền đạt ý nghĩa của một biểu tượng bằng cách xen kẽ các thông báo của người dùng và trợ lý:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"2 🦜 2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
    "    {\"role\": \"user\", \"content\": \"2 🦜 3\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
    "    {\"role\": \"user\", \"content\": \"3 🦜 4\"},\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu ra có cấu trúc thường sử dụng lệnh gọi công cụ ẩn. Điều này thường liên quan đến việc tạo ra các thông báo AI chứa lệnh gọi công cụ, cũng như các thông báo công cụ chứa kết quả của lệnh gọi công cụ. Một chuỗi thông báo trông như thế nào trong trường hợp này? Các nhà cung cấp mô hình trò chuyện khác nhau áp đặt các yêu cầu khác nhau đối với chuỗi thông báo hợp lệ. Một số sẽ chấp nhận chuỗi thông báo (lặp lại) có dạng: Thông báo người dùng Thông báo AI có lệnh gọi công cụ Thông báo công cụ có kết quả\n",
    "\n",
    "Hãy thử điều này. Chúng ta có thể chuyển đổi các cặp chuỗi đầu vào và các đối tượng Pydantic mong muốn thành một chuỗi các tin nhắn có thể được cung cấp cho một mô hình trò chuyện. Về cơ bản, LangChain sẽ định dạng các lệnh gọi công cụ theo định dạng bắt buộc của từng nhà cung cấp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NGOVANUC\\AppData\\Local\\Temp\\ipykernel_23780\\4046823788.py:16: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
      "  messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "examples = [\n",
    "    (\"The ocean is vast and blue. It's more than 20,000 feet deep.\", Data(people=[])),\n",
    "    ( \"Fiona traveled far from France to Spain.\", Data(people=[Person(name='Fiona', height_in_meters=None, hair_color=None)])),\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for txt, tool_call in examples:\n",
    "    if tool_call.people:\n",
    "        # This final message is optional for some providers\n",
    "        ai_response = \"Detected people.\"\n",
    "    else:\n",
    "        ai_response = \"Detected no people\"\n",
    "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n",
    "    # Hàm tool_example_to_messages chuyển đổi (các) ví dụ (example) thành (các) tin nhắn (message)\n",
    "    # để có thể tương thích khi đưa vào mô hình LLM\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        HumanMessage: \"The ocean is vast and blue. It's more than 20,000 feet deep.\"\n",
    "        AIMessage: { \"name\": None, \"height_in_meters\": None, \"hair_color\": None }\n",
    "        ToolMessage: \"Tool call executed successfully\"\n",
    "        AIMessage (optional): \"Detected no people.\"\n",
    "    \"\"\"\n",
    "    # Hàm này đặc biệt hữu ích khi làm việc với LLM có hỗ trợ tool calling và cần phản hồi có cấu trúc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The ocean is vast and blue. It's more than 20,000 feet deep.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (2bd5a52a-fb43-4f1e-b252-9ea596684d85)\n",
      " Call ID: 2bd5a52a-fb43-4f1e-b252-9ea596684d85\n",
      "  Args:\n",
      "    people: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected no people\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Fiona traveled far from France to Spain.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (545455fe-78a0-49ae-b525-987e1228e804)\n",
      " Call ID: 545455fe-78a0-49ae-b525-987e1228e804\n",
      "  Args:\n",
      "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected people.\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy so sánh hiệu suất có và không có các thông báo này. Ví dụ, hãy truyền một thông báo mà chúng ta không muốn trích xuất bất kỳ người nào:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Astronomy Enthusiast', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_no_extraction = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"The solar system is large, but earth has only 1 moon.\",\n",
    "}\n",
    "\n",
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "structured_llm.invoke([message_no_extraction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ví dụ này, mô hình có thể tạo ra các bản ghi sai về mọi người. Vì các ví dụ ít cảnh của chúng tôi chứa các ví dụ về \"phủ định\", chúng tôi khuyến khích mô hình hoạt động chính xác trong trường hợp này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(messages + [message_no_extraction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xem hướng dẫn này để biết thêm chi tiết về quy trình trích xuất với các ví dụ tham khảo, bao gồm cách kết hợp các mẫu nhắc nhở và tùy chỉnh việc tạo các thông báo ví dụ.\n",
    "\n",
    "    https://python.langchain.com/docs/how_to/extraction_examples/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ bạn đã hiểu những điều cơ bản về trích xuất bằng LangChain, bạn đã sẵn sàng để tiến hành các hướng dẫn còn lại: \n",
    "- Thêm ví dụ: Chi tiết hơn về việc sử dụng các ví dụ tham khảo để cải thiện hiệu suất.\n",
    "- Xử lý văn bản dài: Bạn nên làm gì nếu văn bản không phù hợp với cửa sổ ngữ cảnh của LLM?\n",
    "- Sử dụng phương pháp phân tích cú pháp: Sử dụng phương pháp dựa trên lời nhắc để trích xuất bằng các mô hình không hỗ trợ công cụ/chức năng\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
