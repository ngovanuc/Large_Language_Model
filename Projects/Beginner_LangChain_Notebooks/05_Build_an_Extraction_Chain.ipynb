{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction: Extract structured data from text and other unstructured media using chat models and few-shot examples.\n",
    "\n",
    "Trong h∆∞·ªõng d·∫´n n√†y, ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng c√°c t√≠nh nƒÉng g·ªçi c√¥ng c·ª• (tool calling) c·ªßa m√¥ h√¨nh tr√≤ chuy·ªán ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin c√≥ c·∫•u tr√∫c t·ª´ vƒÉn b·∫£n kh√¥ng c√≥ c·∫•u tr√∫c. Ch√∫ng t√¥i c≈©ng s·∫Ω tr√¨nh b√†y c√°ch s·ª≠ d·ª•ng nh·∫Øc nh·ªü (prompt) √≠t l·∫ßn trong b·ªëi c·∫£nh n√†y ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t.\n",
    "\n",
    "## Content\n",
    "1. Setup\n",
    "2. LangSmith\n",
    "3. The Schema\n",
    "4. The Extractor\n",
    "5. Multiple Entities\n",
    "6. Reference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRAING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ti√™n, ch√∫ng ta c·∫ßn m√¥ t·∫£ th√¥ng tin n√†o ch√∫ng ta mu·ªën tr√≠ch xu·∫•t t·ª´ ‚Äã‚ÄãvƒÉn b·∫£n. Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng Pydantic ƒë·ªÉ x√°c ƒë·ªãnh m·ªôt l∆∞·ª£c ƒë·ªì v√≠ d·ª• ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin c√° nh√¢n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    # Doc-string for the entity person\n",
    "    # This doc-string is sent to LLM as the description of the schema person,\n",
    "    # and it can help to improve axtraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field as an `optional` -- this allows the model to decline to extract it\n",
    "    # 2. Each field has a `description` -- this description is used by LLM\n",
    "    # Having a good description can help improve extraction result\n",
    "\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"The color of the person's hair if know\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height measured in meters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C√≥ hai c√°ch th·ª±c h√†nh t·ªët nh·∫•t khi x√°c ƒë·ªãnh l∆∞·ª£c ƒë·ªì: \n",
    "\n",
    "    1. Ghi l·∫°i c√°c thu·ªôc t√≠nh v√† ch√≠nh l∆∞·ª£c ƒë·ªì: Th√¥ng tin n√†y ƒë∆∞·ª£c g·ª≠i ƒë·∫øn LLM v√† ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng tr√≠ch xu·∫•t th√¥ng tin.\n",
    "    2. ƒê·ª´ng √©p LLM ph·∫£i t·∫°o th√¥ng tin! ·ªû tr√™n, ch√∫ng t√¥i ƒë√£ s·ª≠ d·ª•ng Optional cho c√°c thu·ªôc t√≠nh cho ph√©p LLM ƒë∆∞a ra None n·∫øu kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi.\n",
    "\n",
    "ƒê·ªÉ c√≥ hi·ªáu su·∫•t t·ªët nh·∫•t, h√£y ghi l·∫°i l∆∞·ª£c ƒë·ªì th·∫≠t k·ªπ v√† ƒë·∫£m b·∫£o m√¥ h√¨nh kh√¥ng b·ªã √©p ph·∫£i tr·∫£ v·ªÅ k·∫øt qu·∫£ n·∫øu kh√¥ng c√≥ th√¥ng tin n√†o c·∫ßn tr√≠ch xu·∫•t trong vƒÉn b·∫£n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H√£y t·∫°o m·ªôt tr√¨nh tr√≠ch xu·∫•t th√¥ng tin b·∫±ng c√°ch s·ª≠ d·ª•ng l∆∞·ª£c ƒë·ªì ƒë√£ x√°c ƒë·ªãnh ·ªü tr√™n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from pydantic import BaseModel\n",
    "from pydantic import Field\n",
    "\n",
    "# Define a custom prompt to provide instructions and any additional context.\n",
    "# 1) You can add examples into the prompt template to improve extraction quality\n",
    "# 2) Introduce additional parameters to take context into account (e.g., include metadata\n",
    "#    about the document from which the text was extracted.)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm.\"\n",
    "            \"Only extract relevant information from the text.\"\n",
    "            \"If you do not know the value of an attribute asked to extract,\"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ch√∫ng ta c·∫ßn s·ª≠ d·ª•ng m·ªôt m√¥ h√¨nh h·ªó tr·ª£ vi·ªác g·ªçi h√†m/c√¥ng c·ª•."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Alan Smith', hair_color='blond', height_in_meters='1.83')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Alan Smith is 6 feet tall and has blond hair.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multiple Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong h·∫ßu h·∫øt c√°c tr∆∞·ªùng h·ª£p, b·∫°n n√™n tr√≠ch xu·∫•t danh s√°ch c√°c th·ª±c th·ªÉ thay v√¨ m·ªôt th·ª±c th·ªÉ duy nh·∫•t. ƒêi·ªÅu n√†y c√≥ th·ªÉ d·ªÖ d√†ng ƒë·∫°t ƒë∆∞·ª£c b·∫±ng c√°ch s·ª≠ d·ª•ng pydantic b·∫±ng c√°ch l·ªìng c√°c m√¥ h√¨nh v√†o nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    # ^ Doc-string for the entity Person.\n",
    "    # This doc-string is sent to the LLM as the description of the schema Person,\n",
    "    # and it can help to improve extraction results.\n",
    "\n",
    "    # Note that:\n",
    "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
    "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
    "    # Having a good description can help improve extraction results.\n",
    "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
    "    hair_color: Optional[str] = Field(default=None, description=\"The color of the person's hair if known\")\n",
    "    height_in_meters: Optional[str] = Field(default=None, description=\"Height measured in meters\")\n",
    "\n",
    "\n",
    "class Data(BaseModel):\n",
    "    \"\"\"Extracted data about people.\"\"\"\n",
    "    # Creates a model so that we can extract multiple entities.\n",
    "    people: List[Person]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    IMPORTANT\n",
    "K·∫øt qu·∫£ tr√≠ch xu·∫•t c√≥ th·ªÉ kh√¥ng ho√†n h·∫£o ·ªü ƒë√¢y. ƒê·ªçc ti·∫øp ƒë·ªÉ xem c√°ch s·ª≠ d·ª•ng V√≠ d·ª• tham kh·∫£o ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng tr√≠ch xu·∫•t v√† xem h∆∞·ªõng d·∫´n c√°ch tr√≠ch xu·∫•t c·ªßa ch√∫ng t√¥i ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.\n",
    "\n",
    "    https://python.langchain.com/docs/how_to/#extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "text = \"My name is Jeff, my hair is black and i am 6 feet tall. Anna has the same color hair as me.\"\n",
    "prompt = prompt_template.invoke({\"text\": text})\n",
    "structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tip\n",
    "\n",
    "Khi l∆∞·ª£c ƒë·ªì h·ªó tr·ª£ vi·ªác tr√≠ch xu·∫•t nhi·ªÅu th·ª±c th·ªÉ, n√≥ c≈©ng cho ph√©p m√¥ h√¨nh kh√¥ng tr√≠ch xu·∫•t b·∫•t k·ª≥ th·ª±c th·ªÉ n√†o n·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan n√†o trong vƒÉn b·∫£n b·∫±ng c√°ch cung c·∫•p m·ªôt danh s√°ch tr·ªëng. Th√¥ng th∆∞·ªùng, ƒë√¢y l√† m·ªôt ƒëi·ªÅu t·ªët! N√≥ cho ph√©p ch·ªâ ƒë·ªãnh c√°c thu·ªôc t√≠nh b·∫Øt bu·ªôc tr√™n m·ªôt th·ª±c th·ªÉ m√† kh√¥ng nh·∫•t thi·∫øt bu·ªôc m√¥ h√¨nh ph·∫£i ph√°t hi·ªán th·ª±c th·ªÉ n√†y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H√†nh vi c·ªßa c√°c ·ª©ng d·ª•ng LLM c√≥ th·ªÉ ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn b·∫±ng c√°ch s·ª≠ d·ª•ng l·ªùi nh·∫Øc √≠t l·∫ßn. ƒê·ªëi v·ªõi c√°c m√¥ h√¨nh tr√≤ chuy·ªán, ƒëi·ªÅu n√†y c√≥ th·ªÉ di·ªÖn ra d∆∞·ªõi d·∫°ng m·ªôt chu·ªói c√°c c·∫∑p th√¥ng b√°o ƒë·∫ßu v√†o v√† ph·∫£n h·ªìi th·ªÉ hi·ªán c√°c h√†nh vi mong mu·ªën. V√≠ d·ª•, ch√∫ng ta c√≥ th·ªÉ truy·ªÅn ƒë·∫°t √Ω nghƒ©a c·ªßa m·ªôt bi·ªÉu t∆∞·ª£ng b·∫±ng c√°ch xen k·∫Ω c√°c th√¥ng b√°o c·ªßa ng∆∞·ªùi d√πng v√† tr·ª£ l√Ω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"2 ü¶ú 2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"4\"},\n",
    "    {\"role\": \"user\", \"content\": \"2 ü¶ú 3\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"5\"},\n",
    "    {\"role\": \"user\", \"content\": \"3 ü¶ú 4\"},\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ra c√≥ c·∫•u tr√∫c th∆∞·ªùng s·ª≠ d·ª•ng l·ªánh g·ªçi c√¥ng c·ª• ·∫©n. ƒêi·ªÅu n√†y th∆∞·ªùng li√™n quan ƒë·∫øn vi·ªác t·∫°o ra c√°c th√¥ng b√°o AI ch·ª©a l·ªánh g·ªçi c√¥ng c·ª•, c≈©ng nh∆∞ c√°c th√¥ng b√°o c√¥ng c·ª• ch·ª©a k·∫øt qu·∫£ c·ªßa l·ªánh g·ªçi c√¥ng c·ª•. M·ªôt chu·ªói th√¥ng b√°o tr√¥ng nh∆∞ th·∫ø n√†o trong tr∆∞·ªùng h·ª£p n√†y? C√°c nh√† cung c·∫•p m√¥ h√¨nh tr√≤ chuy·ªán kh√°c nhau √°p ƒë·∫∑t c√°c y√™u c·∫ßu kh√°c nhau ƒë·ªëi v·ªõi chu·ªói th√¥ng b√°o h·ª£p l·ªá. M·ªôt s·ªë s·∫Ω ch·∫•p nh·∫≠n chu·ªói th√¥ng b√°o (l·∫∑p l·∫°i) c√≥ d·∫°ng: Th√¥ng b√°o ng∆∞·ªùi d√πng Th√¥ng b√°o AI c√≥ l·ªánh g·ªçi c√¥ng c·ª• Th√¥ng b√°o c√¥ng c·ª• c√≥ k·∫øt qu·∫£\n",
    "\n",
    "H√£y th·ª≠ ƒëi·ªÅu n√†y. Ch√∫ng ta c√≥ th·ªÉ chuy·ªÉn ƒë·ªïi c√°c c·∫∑p chu·ªói ƒë·∫ßu v√†o v√† c√°c ƒë·ªëi t∆∞·ª£ng Pydantic mong mu·ªën th√†nh m·ªôt chu·ªói c√°c tin nh·∫Øn c√≥ th·ªÉ ƒë∆∞·ª£c cung c·∫•p cho m·ªôt m√¥ h√¨nh tr√≤ chuy·ªán. V·ªÅ c∆° b·∫£n, LangChain s·∫Ω ƒë·ªãnh d·∫°ng c√°c l·ªánh g·ªçi c√¥ng c·ª• theo ƒë·ªãnh d·∫°ng b·∫Øt bu·ªôc c·ªßa t·ª´ng nh√† cung c·∫•p.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NGOVANUC\\AppData\\Local\\Temp\\ipykernel_23780\\4046823788.py:16: LangChainBetaWarning: The function `tool_example_to_messages` is in beta. It is actively being worked on, so the API may change.\n",
      "  messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.utils.function_calling import tool_example_to_messages\n",
    "\n",
    "examples = [\n",
    "    (\"The ocean is vast and blue. It's more than 20,000 feet deep.\", Data(people=[])),\n",
    "    ( \"Fiona traveled far from France to Spain.\", Data(people=[Person(name='Fiona', height_in_meters=None, hair_color=None)])),\n",
    "]\n",
    "\n",
    "messages = []\n",
    "\n",
    "for txt, tool_call in examples:\n",
    "    if tool_call.people:\n",
    "        # This final message is optional for some providers\n",
    "        ai_response = \"Detected people.\"\n",
    "    else:\n",
    "        ai_response = \"Detected no people\"\n",
    "    messages.extend(tool_example_to_messages(txt, [tool_call], ai_response=ai_response))\n",
    "    # H√†m tool_example_to_messages chuy·ªÉn ƒë·ªïi (c√°c) v√≠ d·ª• (example) th√†nh (c√°c) tin nh·∫Øn (message)\n",
    "    # ƒë·ªÉ c√≥ th·ªÉ t∆∞∆°ng th√≠ch khi ƒë∆∞a v√†o m√¥ h√¨nh LLM\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        HumanMessage: \"The ocean is vast and blue. It's more than 20,000 feet deep.\"\n",
    "        AIMessage: { \"name\": None, \"height_in_meters\": None, \"hair_color\": None }\n",
    "        ToolMessage: \"Tool call executed successfully\"\n",
    "        AIMessage (optional): \"Detected no people.\"\n",
    "    \"\"\"\n",
    "    # H√†m n√†y ƒë·∫∑c bi·ªát h·ªØu √≠ch khi l√†m vi·ªác v·ªõi LLM c√≥ h·ªó tr·ª£ tool calling v√† c·∫ßn ph·∫£n h·ªìi c√≥ c·∫•u tr√∫c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "The ocean is vast and blue. It's more than 20,000 feet deep.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (2bd5a52a-fb43-4f1e-b252-9ea596684d85)\n",
      " Call ID: 2bd5a52a-fb43-4f1e-b252-9ea596684d85\n",
      "  Args:\n",
      "    people: []\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected no people\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Fiona traveled far from France to Spain.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  Data (545455fe-78a0-49ae-b525-987e1228e804)\n",
      " Call ID: 545455fe-78a0-49ae-b525-987e1228e804\n",
      "  Args:\n",
      "    people: [{'name': 'Fiona', 'hair_color': None, 'height_in_meters': None}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "You have correctly called this tool.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Detected people.\n"
     ]
    }
   ],
   "source": [
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H√£y so s√°nh hi·ªáu su·∫•t c√≥ v√† kh√¥ng c√≥ c√°c th√¥ng b√°o n√†y. V√≠ d·ª•, h√£y truy·ªÅn m·ªôt th√¥ng b√°o m√† ch√∫ng ta kh√¥ng mu·ªën tr√≠ch xu·∫•t b·∫•t k·ª≥ ng∆∞·ªùi n√†o:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[Person(name='Astronomy Enthusiast', hair_color=None, height_in_meters=None)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_no_extraction = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"The solar system is large, but earth has only 1 moon.\",\n",
    "}\n",
    "\n",
    "structured_llm = llm.with_structured_output(schema=Data)\n",
    "structured_llm.invoke([message_no_extraction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong v√≠ d·ª• n√†y, m√¥ h√¨nh c√≥ th·ªÉ t·∫°o ra c√°c b·∫£n ghi sai v·ªÅ m·ªçi ng∆∞·ªùi. V√¨ c√°c v√≠ d·ª• √≠t c·∫£nh c·ªßa ch√∫ng t√¥i ch·ª©a c√°c v√≠ d·ª• v·ªÅ \"ph·ªß ƒë·ªãnh\", ch√∫ng t√¥i khuy·∫øn kh√≠ch m√¥ h√¨nh ho·∫°t ƒë·ªông ch√≠nh x√°c trong tr∆∞·ªùng h·ª£p n√†y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(people=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(messages + [message_no_extraction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xem h∆∞·ªõng d·∫´n n√†y ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt v·ªÅ quy tr√¨nh tr√≠ch xu·∫•t v·ªõi c√°c v√≠ d·ª• tham kh·∫£o, bao g·ªìm c√°ch k·∫øt h·ª£p c√°c m·∫´u nh·∫Øc nh·ªü v√† t√πy ch·ªânh vi·ªác t·∫°o c√°c th√¥ng b√°o v√≠ d·ª•.\n",
    "\n",
    "    https://python.langchain.com/docs/how_to/extraction_examples/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√¢y gi·ªù b·∫°n ƒë√£ hi·ªÉu nh·ªØng ƒëi·ªÅu c∆° b·∫£n v·ªÅ tr√≠ch xu·∫•t b·∫±ng LangChain, b·∫°n ƒë√£ s·∫µn s√†ng ƒë·ªÉ ti·∫øn h√†nh c√°c h∆∞·ªõng d·∫´n c√≤n l·∫°i: \n",
    "- Th√™m v√≠ d·ª•: Chi ti·∫øt h∆°n v·ªÅ vi·ªác s·ª≠ d·ª•ng c√°c v√≠ d·ª• tham kh·∫£o ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t.\n",
    "- X·ª≠ l√Ω vƒÉn b·∫£n d√†i: B·∫°n n√™n l√†m g√¨ n·∫øu vƒÉn b·∫£n kh√¥ng ph√π h·ª£p v·ªõi c·ª≠a s·ªï ng·ªØ c·∫£nh c·ªßa LLM?\n",
    "- S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ph√¢n t√≠ch c√∫ ph√°p: S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª±a tr√™n l·ªùi nh·∫Øc ƒë·ªÉ tr√≠ch xu·∫•t b·∫±ng c√°c m√¥ h√¨nh kh√¥ng h·ªó tr·ª£ c√¥ng c·ª•/ch·ª©c nƒÉng\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
