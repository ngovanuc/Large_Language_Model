{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple LLM application with chat models and prompt templates\n",
    "This application will translate text from English into another language.\n",
    "## Content\n",
    "1. Setup\n",
    "2. LangSmith\n",
    "3. Using Language Models\n",
    "4. Streaming\n",
    "5. Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tracing (LangSmith)\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.environ.get('GROQ_API_KEY'):\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider='groq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Bonjour ! Parlez-vous anglais ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 29, 'total_tokens': 37, 'completion_time': 0.006666667, 'prompt_time': 0.004062394, 'queue_time': 0.018330713999999998, 'total_time': 0.010729061}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-3e93dfc0-b7eb-4efa-b1a6-006039fa5524-0', usage_metadata={'input_tokens': 29, 'output_tokens': 8, 'total_tokens': 37})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English to French\"),\n",
    "    HumanMessage(\"Hi! Do you speak English?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIPS:\n",
    "\n",
    "Nếu chúng ta đã bật LangSmith, chúng ta có thể thấy rằng lần chạy này được ghi vào LangSmith và có thể thấy dấu vết LangSmith. Dấu vết LangSmith báo cáo thông tin sử dụng mã thông báo, độ trễ, các tham số mô hình chuẩn (như nhiệt độ) và các thông tin khác."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38, 'completion_time': 0.021666667, 'prompt_time': 0.001823676, 'queue_time': 0.020508483, 'total_time': 0.023490343}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-1560e71f-3da3-41be-889e-8634026f97f2-0' usage_metadata={'input_tokens': 12, 'output_tokens': 26, 'total_tokens': 38}\n",
      "Response 2: content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38, 'completion_time': 0.021666667, 'prompt_time': 0.001836466, 'queue_time': 0.017368733, 'total_time': 0.023503133}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-b0697b13-6282-4a83-9dc4-894d1eaff699-0' usage_metadata={'input_tokens': 12, 'output_tokens': 26, 'total_tokens': 38}\n",
      "Response 3: content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 12, 'total_tokens': 38, 'completion_time': 0.021666667, 'prompt_time': 0.001834053, 'queue_time': 0.019068678000000002, 'total_time': 0.02350072}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-662a2978-91ad-41e5-9a99-9be378648a90-0' usage_metadata={'input_tokens': 12, 'output_tokens': 26, 'total_tokens': 38}\n"
     ]
    }
   ],
   "source": [
    "# LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n",
    "print(f'Response 1: {model.invoke(\"Hello!\")}')\n",
    "print(f'Response 2: {model.invoke([{'role': 'user', 'content': 'Hello!'}])}')\n",
    "print(f'Response 3: {model.invoke([HumanMessage('Hello!')])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì các mô hình trò chuyện là Runnables, chúng hiển thị một giao diện chuẩn bao gồm các chế độ gọi async và streaming. Điều này cho phép chúng ta stream các token riêng lẻ từ một mô hình trò chuyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Bonjour| !| Par|lez|-vous| anglais| ?||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt (lời nhắc/thông điệp)\n",
    "- Prompt là những thông điệp mà chúng ta đưa vào mô hình\n",
    "- Prompt được xây dựng bằng cách kết hợp đầu vào của người dùng và logic ứng dụng\n",
    "- Logic ứng dụng này thường lấy đầu vào thô của người dùng và chuyển đổi nó thành một danh sách các thông điệp sẵn sàng để truyền đến mô hình\n",
    "- Cách chuyển đổi phổ biến: thêm một thông điệp hệ thống hoặc định dạng một mẫu với đầu vào của người dùng\n",
    "\n",
    "Mẫu nhắc nhở (prompt templates) là một khái niệm trong LangChain được thiết kế để hỗ trợ quá trình chuyển đổi này. Chúng tiếp nhận dữ liệu thô từ người dùng và trả về dữ liệu (lời nhắc) đã sẵn sàng để chuyển vào mô hình ngôn ngữ.\n",
    "\n",
    "Let's create a prompt template here. It will take in two user variables:\n",
    "\n",
    "    - language: The language to translate text into\n",
    "    - text: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ChatPromptTemplate có thể hỗ trợ nhiều vai trò trong một template\n",
    "- truyền vào một tham số language thành tin nhắn hệ thống (SystemMessage) và văn bản người dùng thành tin nhắn người dùng (text). Chúng ta có thể thử nghiệm các mẫu khác nhau để xem nó có thể tự làm gì!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into French', additional_kwargs={}, response_metadata={}), HumanMessage(content='Xin chào!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"French\", \"text\": \"Hi!\"})\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể thấy rằng nó trả về một ChatPromptValue bao gồm hai tin nhắn. Nếu chúng ta muốn truy cập trực tiếp vào các tin nhắn, chúng ta thực hiện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Xin chào!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='Bonjour!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 27, 'total_tokens': 30, 'completion_time': 0.0025, 'prompt_time': 0.003916825, 'queue_time': 0.019110424, 'total_time': 0.006416825}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-21406647-18b6-4ecd-bf96-9376447b1343-0' usage_metadata={'input_tokens': 27, 'output_tokens': 3, 'total_tokens': 30}\n",
      "Content: Bonjour!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(f'Response: {response}')\n",
    "print(f'Content: {response.content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
