{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groq LPU Inference Engine Tutorial\n",
    "## Content\n",
    "1. OpenAI\n",
    "2. Groq LPU Inference Engine là gì?\n",
    "3. Step-by-step guid to using Groq Python API\n",
    "    3.1. Setting up\n",
    "    3.2. Basic completion and chat\n",
    "    3.3. Streaming chat completion\n",
    "    3.4. Async chat completion\n",
    "    3.5. Streaming an Async chat completion\n",
    "4. Building with AI application with Groq API and LlamaIndex\n",
    "    4.2. Setting up the LLM using Groq\n",
    "    4.4. Global settings configuration\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI API provides a wide range of features and models. It offers:\n",
    "- Embedding models.\n",
    "- Access to text generation models like GPT-4o and GPT-4 Turbo.\n",
    "- Code interpreter and file search.\n",
    "- Ability to finetune the models on a custom dataset.\n",
    "- Access to image generation models.\n",
    "- Audio model for transcription, translation, and text-to-speech.\n",
    "- Vision model to understand images.\n",
    "- Function calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Groq LPU Inference Engine là gì?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Groq LPU (Language Processing Unit) là một bộ xử lý mới chuyên xử lý các tác vụ nặng về tính toán có tính tuần tự\n",
    "- Đặc biệt trong tạo phản hồi các mô hình ngôn ngữ tự nhiên\n",
    "- So với CPU và GPU, LPU có khả năng tính toán cao hơn, giúp tăng tốc độ tạo văn bản và giảm tắc nghẽn bộ nhớ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Step-by-step guid to using Groq Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001D16B60C140>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/eb/b6/b9dd55845e86d664531caaf57edcbd70b0de8e87c1df72233a082d566b7f/groq-0.19.0-py3-none-any.whl.metadata\n"
     ]
    }
   ],
   "source": [
    "%pip install groq -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Basic completion and chatchat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tạo một api key\n",
    "- Sinh văn bản sử dụng chat completion function\n",
    "- Truyền vào tên model và mesage\n",
    "- Chuyển thành markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Neural networks! One of the most fascinating and powerful tools in the field of machine learning.\n",
       "\n",
       "A neural network is a machine learning model inspired by the structure and function of the human brain. It's a complex system of interconnected nodes or \"neurons\" that process and transmit information.\n",
       "\n",
       "Here's a high-level overview of how neural networks work:\n",
       "\n",
       "**Architecture:**\n",
       "\n",
       "A neural network consists of three types of layers:\n",
       "\n",
       "1. **Input Layer:** This layer receives the input data, which could be images, sound waves, text, or any other type of data.\n",
       "2. **Hidden Layers:** These layers, also known as \"hidden neurons\" or \"feature detectors,\" process the input data. They apply complex transformations to the input data, allowing the network to learn and represent more abstract features.\n",
       "3. **Output Layer:** This layer generates the final output of the network based on the input and the transformations applied by the hidden layers.\n",
       "\n",
       "**How it works:**\n",
       "\n",
       "The process can be broken down into three stages:\n",
       "\n",
       "**Stage 1: Forward Propagation:**\n",
       "\n",
       "1. The input data is fed into the input layer.\n",
       "2. The input data flows through the hidden layers, where each node applies an activation function to the weighted sum of its inputs. This produces an output that is passed to the next layer.\n",
       "3. The output from the hidden layers is fed into the output layer, where the final prediction or classification is made.\n",
       "\n",
       "**Stage 2: Error Calculation:**\n",
       "\n",
       "1. The difference between the network's output and the actual true output is calculated. This difference is known as the \"loss\" or \"error.\"\n",
       "2. The error is propagated backwards through the network, adjusting the weights and biases of each node to minimize the loss.\n",
       "\n",
       "**Stage 3: Backpropagation and Optimization:**\n",
       "\n",
       "1. The error is backpropagated through the network, layer by layer, to compute the gradients of the loss with respect to each node's weights and biases.\n",
       "2. The gradients are used to update the weights and biases using an optimization algorithm, such as stochastic gradient descent (SGD), Adam, or RMSProp.\n",
       "3. The network is trained by repeating the forward propagation, error calculation, and backpropagation steps multiple times, adjusting the weights and biases to minimize the loss.\n",
       "\n",
       "**Key Concepts:**\n",
       "\n",
       "* **Activation Functions:** These introduce non-linearity into the network, allowing it to learn and represent more complex relationships between inputs and outputs. Common examples include sigmoid, ReLU, and tanh.\n",
       "* **Weight Updates:** The process of adjusting the weights and biases of each node based on the error and gradients computed during backpropagation.\n",
       "* **Overfitting:** When a network becomes too complex and starts to fit the noise in the training data rather than the underlying patterns. Regularization techniques, such as dropout and L1/L2 regularization, can help prevent overfitting.\n",
       "\n",
       "**Types of Neural Networks:**\n",
       "\n",
       "* **Feedforward Networks:** The simplest type of neural network, where the data flows only in one direction, from input layer to output layer.\n",
       "* **Recurrent Neural Networks (RNNs):** Designed to handle sequential data, such as speech, text, or time series data. RNNs have feedback connections, allowing the data to flow in a loop.\n",
       "* **Convolutional Neural Networks (CNNs):** Specifically designed for image and signal processing tasks. CNNs use convolutional and pooling layers to extract features from images.\n",
       "\n",
       "This is a basic overview of how neural networks work. If you have specific questions or want to dive deeper into any of these topics, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# os.environ['GROQ_API_KEY'] = insert_your_groq_api_key\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get('insert_your_groq_api_key')\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a professional Data Scientist.'},\n",
    "        {'role': 'user', 'content': 'Can you explain how the neural networks work?'}\n",
    "    ],\n",
    "    model='llama3-70b-8192'\n",
    ")\n",
    "Markdown(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Streaming chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My young friend, the meaning of life. This is a question that has puzzled seekers for centuries. As a monk from Thailand, I have dedicated my life to understanding the teachings of the Buddha and finding answers to this very question.\n",
      "\n",
      "You see, in Buddhism, we believe that the meaning of life is not something that can be found outside of ourselves, but rather it is something that we must discover within. It is a journey of self-discovery, of understanding the nature of reality and our place within it.\n",
      "\n",
      "The Buddha taught that the root of suffering is ignorance, and that the path to enlightenment is through the development of wisdom. This wisdom is not just intellectual understanding, but a deep, experiential understanding of the true nature of reality.\n",
      "\n",
      "So, what is the meaning of life? It is to awaken to the present moment, to let go of our attachments and desires, and to cultivate compassion, wisdom, and mindfulness. It is to live a life of simplicity, humility, and gratitude.\n",
      "\n",
      "In Thailand, we have a saying, \"Sabai sabai,\" which means \"easy, easy.\" It is a reminder to approach life with a sense of ease, to not get caught up in our struggles and worries, but to instead cultivate a sense of peace and contentment.\n",
      "\n",
      "The meaning of life is not something that can be put into words, but it is something that can be experienced. It is the feeling of connection to all living beings, the sense of wonder and awe at the beauty of the world, and the understanding that we are all interconnected.\n",
      "\n",
      "As a monk, I have dedicated my life to cultivating this understanding, to living a life of simplicity and compassion, and to sharing this wisdom with others. And I can tell you, my young friend, that the meaning of life isNone"
     ]
    }
   ],
   "source": [
    "chat_streaming = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a monk from Thailand.'},\n",
    "        {'role': 'user', 'content': 'Can you explain the meaning of life?'}\n",
    "    ],\n",
    "    model='llama3-70b-8192',\n",
    "    temperature=0.3,\n",
    "    max_tokens=360,\n",
    "    top_p=1,\n",
    "    stop=None,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in chat_streaming:\n",
    "    print(chunk.choices[0].delta.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Async chat completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable async API calling, we have to change the structure of the code. We:\n",
    "1. Create an async Groq client with the API key. \n",
    "2. Define the async main function.\n",
    "3. Write the chat completion function with the await keyword.\n",
    "4. Run the main function with the await keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I totally understand. It can be really frustrating when you feel like you're well-prepared, but your nerves get the better of you during the test.\n",
      "\n",
      "Can you tell me more about what happened? What was going through your mind when you started to feel panicked? Was it a specific question that triggered it, or was it more of a general feeling of anxiety?\n",
      "\n",
      "Also, have you experienced test anxiety before, or was this a one-time thing?\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq(\n",
    "    api_key=os.environ.get('insert_your_groq_api_key')\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a psychiatrist helping young minds'},\n",
    "            {'role': 'user', 'content': 'I panicked during the test, even though I knew everything on the test paper.'}\n",
    "        ],\n",
    "        model='llama3-70b-8192',\n",
    "        temperature=0.3,\n",
    "        max_tokens=360,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    print(chat_completion.choices[0].message.content)\n",
    "\n",
    "await main() # for Python file use asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start doing task 1, time process: 4\n",
      "Start doing task 2, time process: 9\n",
      "Start doing task 3, time process: 6\n",
      "Done task 1\n",
      "Done task 3\n",
      "Done task 2\n",
      "Total time processing: 9.008956909179688\n"
     ]
    }
   ],
   "source": [
    "# Code ví dụ cách một hàm async hoạt động\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def task(task_number, time_delay=5):\n",
    "    time_delay = np.random.randint(0, 10)\n",
    "    print(f'Start doing task {task_number}, time process: {time_delay}')\n",
    "    await asyncio.sleep(time_delay)\n",
    "    print(f'Done task {task_number}')\n",
    "\n",
    "async def main():\n",
    "    start = time.time()\n",
    "    tasks = [\n",
    "        task(1),\n",
    "        task(2),\n",
    "        task(3)\n",
    "    ]\n",
    "    await asyncio.gather(*tasks) # Run all at the same time\n",
    "    end = time.time()\n",
    "    print(f'Total time processing: {end-start}')\n",
    "\n",
    "await main() # for Python file use asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Streaming an Async chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I totally understand. It can be really frustrating when you feel like you're well-prepared, but your nerves get the better of you during the test.\n",
      "\n",
      "Can you tell me more about what happened? What did you experience during the test? Was your heart racing, were your hands shaking, or did you feel like you were going to freeze up?\n",
      "\n",
      "Also, have you experienced test anxiety before, or was this a one-time thing?None"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "client = AsyncGroq(\n",
    "    api_key=os.environ.get('insert_your_groq_api_key')\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    chat_streaming = await client.chat.completions.create(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a psychiatrist helping young minds'},\n",
    "            {'role': 'user', 'content': 'I panicked during the test, even though I knew everything on the test paper.'}\n",
    "        ],\n",
    "        model='llama3-70b-8192',\n",
    "        temperature=0.3,\n",
    "        max_tokens=360,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "        stream=True,\n",
    "    )\n",
    "    async for chunk in chat_streaming:  \n",
    "        print(chunk.choices[0].delta.content, end='')\n",
    "\n",
    "await main() # for Python file use asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building with AI application with Groq API and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load the text from a PDF file\n",
    "- convert it into embeddings,\n",
    "- save it into the vector store\n",
    "- convert the vector store into the retriever\n",
    "=> that will be used to build a RAG chat engine with history. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%pip install llama-index\n",
    "%pip install llama-index-llms-groq\n",
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Setting up the LLM using Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(\n",
    "    model='llama3-70b-8192',\n",
    "    api_key=os.environ.get('insert_your_groq_api_key')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Setting up an embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name='mixedbread-ai/mxbai-embed-large-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Global settings configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "de_tools_blog = SimpleDirectoryReader(\n",
    "    '../../Datasets/Application_with_Groq_data/',\n",
    "    required_exts=['.pdf', '.docx']\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='65e988b7-c465-45b6-bf73-3a989b1cb308', embedding=None, metadata={'page_label': '2', 'file_name': '2404.10981v2.pdf', 'file_path': 'e:\\\\LEARNING\\\\Large_Language_Model\\\\Projects\\\\Building_a_Context-aware_ChatGPT_application\\\\..\\\\..\\\\Datasets\\\\Application_with_Groq_data\\\\2404.10981v2.pdf', 'file_type': 'application/pdf', 'file_size': 3055006, 'creation_date': '2025-03-15', 'last_modified_date': '2025-03-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='2 Huang et al.\\nFig. 1. An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the scope\\nof the training data and generates correct results.\\nhinders LLMs’ ability to stay updated. Third, LLMs are susceptible to generating convincing yet\\ninaccurate responses, known as “hallucinations”, which can mislead users.\\nAddressing these challenges is crucial for LLMs to be effectively utilized across various domains.\\nA promising solution is the integration of Retrieval-Augmented Generation (RAG) technology,\\nwhich supplements models by fetching external data in response to queries, thus ensuring more\\naccurate and current outputs. Figure 1 illustrates how RAG can enable ChatGPT to provide precise\\nanswers beyond its initial training data.\\nSince its introduction by Lewis et al. [83] in 2020, RAG has seen rapid development, especially\\nwith the rise of models like ChatGPT. Despite these advancements, there remains a noticeable gap\\nin the literature regarding a comprehensive analysis of the mechanisms underlying RAG and the\\nprogress achieved by subsequent studies. Moreover, the field suffers from fragmented research\\nfocuses and inconsistent terminology for similar methods, leading to confusion. This survey seeks\\nto bridge this gap by offering a structured overview of RAG, categorizing various approaches, and\\nproviding an in-depth understanding of the current research landscape, with a focus on textual\\napplications given their prominence in recent research.\\nTo provide clarity and structure, this paper is organized as follows: Section 2 outlines the\\noverall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and\\ngeneration phases. Sections 3 through 6 explore the core techniques within each phase. Section\\n7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies,\\ndetailing the retrievers and generators used, while Section 9 discusses challenges and future research\\ndirections, extending beyond text-based studies to include multimodal data applications. The paper\\nconcludes with Section 10.\\nOther related surveys provide valuable insights into the evolving RAG landscape from different\\nangles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement,\\ninference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including\\ntext, code, image, and video generation, emphasizing augmented intelligence in generative tasks.\\nMeanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how\\ninteractions between retrievers, language models, and augmentations influence model architectures\\nand applications.\\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from\\nan information retrieval (IR) perspective, identifying key challenges and areas for improvement. We\\ndelve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval\\nand generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG\\nresearch, highlights current limitations, and proposes promising avenues for future exploration.\\n, Vol. 1, No. 1, Article . Publication date: August 2018.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_tools_blog[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorStoreIndex provides the fastest way to create the vector store by loading the documents and building the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 tools mentioned in the provided context information.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(de_tools_blog)\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "response = query_engine.query(\"How many tools are there?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. RAG chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, I can identify some tools and techniques suitable for data processing in the context of Retrieval-Augmented Text Generation in Large Language Models.\n",
      "\n",
      "1. **Graph-based approaches**: Techniques like MEMWALKER and LRUS-CoverTree method are mentioned as innovative approaches to overcome limitations such as context window size in large language models. These methods facilitate efficient indexing and management of large information volumes.\n",
      "\n",
      "2. **Product Quantization (PQ)**: PQ is a method for handling large-scale data, which accelerates searches by segmenting vectors and then clustering each part for quantization. Implementations like PipeRAG, Chameleon system, and AiSAQ are mentioned as improving the efficiency and scalability of PQ in different ways.\n",
      "\n",
      "3. **Locality-sensitive Hashing (LSH)**: LSH is a method that places similar vectors into the same hash bucket with high probability, making it easier to find approximate nearest neighbors. Although it's mentioned as less commonly used in RAG systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over slight loss in accuracy.\n",
      "\n",
      "These tools and techniques are primarily focused on efficient indexing, retrieval, and processing of large-scale data, which is essential for generating high-quality text outputs in Large Language Models.\n",
      "\n",
      "If you have any further questions or would like more information on these tools, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.chat_engine import CondensePlusContextChatEngine\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = CondensePlusContextChatEngine.from_defaults(\n",
    "    index.as_retriever(),\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "response = chat_engine.chat('What tools are suitable for data processing?')\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided documents, I can identify some tools and techniques suitable for data processing in the context of Retrieval-Augmented Text Generation in Large Language Models.\n",
    "\n",
    "1. **Graph-based approaches**: Techniques like MEMWALKER and LRUS-CoverTree method are mentioned as innovative approaches to overcome limitations such as context window size in large language models. These methods facilitate efficient indexing and management of large information volumes.\n",
    "\n",
    "2. **Product Quantization (PQ)**: PQ is a method for handling large-scale data, which accelerates searches by segmenting vectors and then clustering each part for quantization. Implementations like PipeRAG, Chameleon system, and AiSAQ are mentioned as improving the efficiency and scalability of PQ in different ways.\n",
    "\n",
    "3. **Locality-sensitive Hashing (LSH)**: LSH is a method that places similar vectors into the same hash bucket with high probability, making it easier to find approximate nearest neighbors. Although it's mentioned as less commonly used in RAG systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios where speed is prioritized over slight loss in accuracy.\n",
    "\n",
    "These tools and techniques are primarily focused on efficient indexing, retrieval, and processing of large-scale data, which is essential for generating high-quality text outputs in Large Language Models.\n",
    "\n",
    "If you have any further questions or would like more information on these tools, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can try to create a high-level diagram of a data pipeline using the tools and techniques mentioned earlier. Please note that this diagram might not be exhaustive, and the actual implementation may vary depending on the specific use case and requirements.\n",
      "\n",
      "Here's a possible data pipeline diagram:\n",
      "\n",
      "```\n",
      "                                      +---------------+\n",
      "                                      |  Input Data  |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Preprocessing  |\n",
      "                                      |  (Tokenization,  |\n",
      "                                      |   Stopword removal, |\n",
      "                                      |   etc.)          |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Vectorization  |\n",
      "                                      |  (Dense vector    |\n",
      "                                      |   generation using  |\n",
      "                                      |   Large Language  |\n",
      "                                      |   Models)          |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Product        |\n",
      "                                      |  Quantization (PQ)|\n",
      "                                      |  (Segmentation,  |\n",
      "                                      |   Clustering, and  |\n",
      "                                      |   Quantization)    |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Graph-based    |\n",
      "                                      |  Indexing (e.g., |\n",
      "                                      |   MEMWALKER,     |\n",
      "                                      |   LRUS-CoverTree)|\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Locality-      |\n",
      "                                      |  sensitive Hashing|\n",
      "                                      |  (LSH)          |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Query          |\n",
      "                                      |  Manipulation   |\n",
      "                                      |  (Query Expansion,|\n",
      "                                      |   Query Reformulation,|\n",
      "                                      |   Prompt-based Rewriting)|\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Retrieval      |\n",
      "                                      |  (Approximate    |\n",
      "                                      |   Nearest Neighbor|\n",
      "                                      |   Search using    |\n",
      "                                      |   Graph, PQ, or LSH)|\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Post-processing|\n",
      "                                      |  (Ranking, Filtering,|\n",
      "                                      |   etc.)          |\n",
      "                                      +---------------+\n",
      "                                             |\n",
      "                                             |\n",
      "                                             v\n",
      "                                      +---------------+\n",
      "                                      |  Output         |\n",
      "                                      |  (Generated Text) |\n",
      "                                      +---------------+\n",
      "```\n",
      "\n",
      "This diagram illustrates a possible data pipeline that incorporates the mentioned tools and techniques:\n",
      "\n",
      "1. Input data is preprocessed (tokenization, stopword removal, etc.).\n",
      "2. The preprocessed data is vectorized using Large Language Models.\n",
      "3. The dense vectors are then processed using Product Quantization (PQ) for efficient indexing.\n",
      "4. Graph-based indexing methods (e.g., MEMWALKER, LRUS-CoverTree) are used to create an index for fast retrieval.\n",
      "5. Locality-sensitive Hashing (LSH) is used as an alternative or complementary approach for fast retrieval.\n",
      "6. Query manipulation techniques (query expansion, query reformulation, prompt-based rewriting) are applied to refine the user query.\n",
      "7. The refined query is then used for retrieval, which involves approximate nearest neighbor search using the graph, PQ, or LSH indexing methods.\n",
      "8. The retrieved results are post-processed (ranking, filtering, etc.) to generate the final output text.\n",
      "\n",
      "Please note that this is a high-level diagram, and the actual implementation may require additional steps, modifications, or variations depending on the specific use case and requirements.\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\n",
    "    \"Can you create a diagram of a data pipeline using these tools?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can try to create a high-level diagram of a data pipeline using the tools and techniques mentioned earlier. Please note that this diagram might not be exhaustive, and the actual implementation may vary depending on the specific use case and requirements.\n",
    "\n",
    "Here's a possible data pipeline diagram:\n",
    "\n",
    "```\n",
    "                                      +---------------+\n",
    "                                      |  Input Data  |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Preprocessing  |\n",
    "                                      |  (Tokenization,  |\n",
    "                                      |   Stopword removal, |\n",
    "                                      |   etc.)          |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Vectorization  |\n",
    "                                      |  (Dense vector    |\n",
    "                                      |   generation using  |\n",
    "                                      |   Large Language  |\n",
    "                                      |   Models)          |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Product        |\n",
    "                                      |  Quantization (PQ)|\n",
    "                                      |  (Segmentation,  |\n",
    "                                      |   Clustering, and  |\n",
    "                                      |   Quantization)    |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Graph-based    |\n",
    "                                      |  Indexing (e.g., |\n",
    "                                      |   MEMWALKER,     |\n",
    "                                      |   LRUS-CoverTree)|\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Locality-      |\n",
    "                                      |  sensitive Hashing|\n",
    "                                      |  (LSH)          |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Query          |\n",
    "                                      |  Manipulation   |\n",
    "                                      |  (Query Expansion,|\n",
    "                                      |   Query Reformulation,|\n",
    "                                      |   Prompt-based Rewriting)|\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Retrieval      |\n",
    "                                      |  (Approximate    |\n",
    "                                      |   Nearest Neighbor|\n",
    "                                      |   Search using    |\n",
    "                                      |   Graph, PQ, or LSH)|\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Post-processing|\n",
    "                                      |  (Ranking, Filtering,|\n",
    "                                      |   etc.)          |\n",
    "                                      +---------------+\n",
    "                                             |\n",
    "                                             |\n",
    "                                             v\n",
    "                                      +---------------+\n",
    "                                      |  Output         |\n",
    "                                      |  (Generated Text) |\n",
    "                                      +---------------+\n",
    "```\n",
    "\n",
    "This diagram illustrates a possible data pipeline that incorporates the mentioned tools and techniques:\n",
    "\n",
    "1. Input data is preprocessed (tokenization, stopword removal, etc.).\n",
    "2. The preprocessed data is vectorized using Large Language Models.\n",
    "3. The dense vectors are then processed using Product Quantization (PQ) for efficient indexing.\n",
    "4. Graph-based indexing methods (e.g., MEMWALKER, LRUS-CoverTree) are used to create an index for fast retrieval.\n",
    "5. Locality-sensitive Hashing (LSH) is used as an alternative or complementary approach for fast retrieval.\n",
    "6. Query manipulation techniques (query expansion, query reformulation, prompt-based rewriting) are applied to refine the user query.\n",
    "7. The refined query is then used for retrieval, which involves approximate nearest neighbor search using the graph, PQ, or LSH indexing methods.\n",
    "8. The retrieved results are post-processed (ranking, filtering, etc.) to generate the final output text.\n",
    "\n",
    "Please note that this is a high-level diagram, and the actual implementation may require additional steps, modifications, or variations depending on the specific use case and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Groq LPU Inference Engine** giúp tăng tốc xử lý mô hình AI, tạo đột phá trong lĩnh vực AI.  \n",
    "✅ **Groq** dù mới nhưng đã gây ấn tượng mạnh trong cộng đồng AI.  \n",
    "✅ Đã tìm hiểu về:  \n",
    "   - **Groq LPU inference engine**  \n",
    "   - **Groq Cloud**  \n",
    "   - **Tích hợp Groq API vào VSCode & Jan AI**  \n",
    "   - **Groq Python package** (có ví dụ code)  \n",
    "   - **Xây dựng AI có khả năng học từ lịch sử chat & tài liệu PDF**  \n",
    "✅ **Bước tiếp theo**: Fine-tuning LLMs với dữ liệu tùy chỉnh (tham khảo hướng dẫn fine-tuning Google Gemma)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
